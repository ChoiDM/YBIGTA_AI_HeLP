{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_loader import train_data_loader, test_data_loader\n",
    "from utils.inference_tools import pred_to_binary, export_csv, making_result\n",
    "from utils.model_stacking import *\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, RidgeClassifier, SGDClassifier, Lars, LassoLars\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 07:08:22\n",
      "Author Name : KHW\n",
      "Model : Stacking\n",
      "Summary : HyperParams tuning with 9 sklearn models\n",
      "\n",
      "\n",
      "Processing [1/3] Image of Positive Patient... (07:08:22)\n",
      ">>> Finished : Voxel Size Resampling (07:08:38)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (07:08:39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [2/3] Image of Positive Patient... (07:08:41)\n",
      ">>> Finished : Voxel Size Resampling (07:08:57)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (07:08:58)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [3/3] Image of Positive Patient... (07:09:00)\n",
      ">>> Finished : Voxel Size Resampling (07:09:16)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (07:09:17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [1/3] Image of Negative Patient... (07:09:20)\n",
      ">>> Finished : Voxel Size Resampling (07:09:35)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (07:09:37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [2/3] Image of Negative Patient... (07:09:40)\n",
      ">>> Finished : Voxel Size Resampling (07:09:57)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (07:09:58)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [3/3] Image of Negative Patient... (07:10:01)\n",
      ">>> Finished : Voxel Size Resampling (07:10:17)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (07:10:18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created X of shape (6, 186) and y of shape (6,) (07:10:20)\n",
      "Processing [1/2] Image of Test Patient... (07:10:20)\n",
      ">>> Finished : Voxel Size Resampling (07:10:36)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>>Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (07:10:37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [2/2] Image of Test Patient... (07:11:39)\n",
      ">>> Finished : Voxel Size Resampling (07:11:56)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>>Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (07:11:58)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    }
   ],
   "source": [
    "# Print Current Time\n",
    "time = str(datetime.datetime.now()).split()[1].split('.')[0]\n",
    "print(\"Start:\", time)\n",
    "\n",
    "\n",
    "# Print Information\n",
    "name = 'KHW'\n",
    "model = 'ML Stacking'\n",
    "summary = 'HyperParams tuning with 10 sklearn models + 4 stacking model'\n",
    "\n",
    "print('Author Name :', name)\n",
    "print('Model :', model)\n",
    "print('Summary :', summary)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Setting\n",
    "path = \"./data\"\n",
    "pos_dir = path+\"/train/positive/\"\n",
    "neg_dir = path+\"/train/negative/\"\n",
    "test_dir = path+'/test/'\n",
    "\n",
    "do_n4 = False\n",
    "do_ws = True\n",
    "do_resample = True\n",
    "\n",
    "do_shuffle_train = True\n",
    "do_shuffle_test = False\n",
    "save_to_disk = False\n",
    "return_patient_num_train = False\n",
    "return_patient_num_test = True\n",
    "\n",
    "\n",
    "# Data Load\n",
    "X_train, y_train = train_data_loader(pos_dir, neg_dir, do_n4, do_ws, do_resample, do_shuffle_train, save_to_disk, return_patient_num_train)\n",
    "X_test, patient_num = test_data_loader(test_dir, do_n4, do_ws, do_resample, do_shuffle_test, save_to_disk, return_patient_num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=4, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Model with Training Data\n",
    "model1 = XGBClassifier(n_jobs=4)\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Save model to file\n",
    "# pickle.dump(model1, open('./data/model/model1.pickle.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Model with Training Data\n",
    "model2 = SVC()\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Save model to file\n",
    "# pickle.dump(model2, open('./data/model/model2.pickle.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=4,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Model with Training Data\n",
    "model3 = LogisticRegression(n_jobs=4)\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Save model to file\n",
    "# pickle.dump(model3, open('./data/model/model3.pickle.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=4,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Model with Training Data\n",
    "model4 = RandomForestClassifier(n_jobs=4)\n",
    "model4.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Save model to file\n",
    "# pickle.dump(mode4l, open('./data/model/model4.pickle.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_scorer(y_true, y_pred, threshold=0.5) :\n",
    "    result = []\n",
    "\n",
    "    for pred in list(y_pred) :\n",
    "        if pred >= threshold :\n",
    "            result.append(1)\n",
    "        else :\n",
    "            result.append(0)\n",
    "            \n",
    "    return fbeta_score(y_true, np.array(result), beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = make_scorer(fbeta_score, beta=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning & CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.5555555555555556\n",
      "Best Params : {'colsample_bytree': 0.5, 'gamma': 1.5, 'learning_rate': 0.01, 'max_depth': 5, 'min_child_weight': 0.5, 'n_estimators': 500, 'probability': True, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "m1_params1 = {\n",
    "    'max_depth' : [5,6,7,8],\n",
    "    'min_child_weight' : [0.5, 1, 5, 10, 15, 20],\n",
    "    'gamma' : [1.5, 2, 2.5, 3.0, 5],\n",
    "    'subsample' : [0.5, 0.6, 0.8, 1.0],\n",
    "    'colsample_bytree' : [0.5, 0.6, 0.8, 1.0],\n",
    "    'probability' : [True],\n",
    "    'learning_rate' : [0.01, 0.05, 0.1],\n",
    "    'n_estimators' : [300, 500, 700]\n",
    "}\n",
    "\n",
    "m1_grid_1 = GridSearchCV(model1, param_grid=m1_params1, scoring=scorer, cv=2, verbose=0, n_jobs=-1)\n",
    "m1_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model1 = m1_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m1_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m1_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.5555555555555556\n",
      "Best Params : {'C': 0.001, 'degree': 2, 'gamma': 0.001, 'probability': True}\n"
     ]
    }
   ],
   "source": [
    "m2_params1 = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100], \n",
    "    'gamma' : [0.001, 0.01, 0.1, 1, 2, 5, 10, 20],\n",
    "    'degree' : [2,3,4],\n",
    "    'probability' : [True]\n",
    "}\n",
    "\n",
    "m2_grid_1 = GridSearchCV(model2, param_grid=m2_params1, scoring=scorer, cv=2, verbose=0, n_jobs=-1)\n",
    "m2_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model2 = m2_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m2_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m2_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.0\n",
      "Best Params : {'C': 0.001, 'max_iter': 100}\n"
     ]
    }
   ],
   "source": [
    "m3_params1 = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'max_iter' : [n for n in range(100,1101, 200)],\n",
    "}\n",
    "\n",
    "m3_grid_1 = GridSearchCV(model3, param_grid=m3_params1, scoring=scorer, cv=2, verbose=0, n_jobs=-1)\n",
    "m3_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model3 = m3_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m3_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m3_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.7407407407407408\n",
      "Best Params : {'max_depth': 6, 'min_samples_leaf': 1, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "m4_params1 = {\n",
    "    'max_depth' : [6, 8, 10, 15, 20, 30, 40, 50],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5,10, 20, 50],\n",
    "    'n_estimators' : [100, 300, 500]\n",
    "}\n",
    "\n",
    "m4_grid_1 = GridSearchCV(model4, param_grid=m4_params1, scoring=scorer, cv=2, verbose=0, n_jobs=-1)\n",
    "m4_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model4 = m4_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m4_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m4_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.1851851851851852\n",
      "Best Params : {'C': 0.001, 'max_iter': 100, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "m5_params1 = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'max_iter' : [n for n in range(100,1101, 200)],\n",
    "    'penalty' : [\"l1\"]\n",
    "}\n",
    "\n",
    "m5_grid_1 = GridSearchCV(model5, param_grid=m5_params1, scoring=scorer, cv=2, verbose=0, n_jobs=-1)\n",
    "m5_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model5 = m5_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m5_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m5_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = RidgeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.5555555555555556\n",
      "Best Params : {'alpha': 0.1, 'max_iter': None}\n"
     ]
    }
   ],
   "source": [
    "m6_params1 = {\n",
    "    'alpha': [0.1, 1, 2, 5, 10, 20, 50, 100],\n",
    "    'max_iter' : [None]+[n for n in range(100,1101, 200)]\n",
    "}\n",
    "\n",
    "m6_grid_1 = GridSearchCV(model6, param_grid=m6_params1, scoring=scorer, cv=2, verbose=0, n_jobs=-1)\n",
    "m6_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model6 = m6_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m6_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m6_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### elasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.5555555555555556\n",
      "Best Params : {'alpha': 0.001, 'l1_ratio': 0.3, 'loss': 'log', 'max_iter': 800, 'penalty': 'elasticnet'}\n"
     ]
    }
   ],
   "source": [
    "m7_params1 = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 2, 5, 10, 20, 50, 100],\n",
    "    'l1_ratio':[0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7], \n",
    "    'max_iter' : [None]+[n for n in range(800, 1601, 200)],\n",
    "    'penalty' : [\"elasticnet\"],\n",
    "    'loss' : [\"log\"]\n",
    "}\n",
    "\n",
    "m7_grid_1 = GridSearchCV(model7, param_grid=m7_params1, scoring=scorer, cv=2, verbose=0, n_jobs=-1)\n",
    "m7_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model7 = m7_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m7_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m7_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = Lars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.4166666666666667\n",
      "Threshold : 0.6\n",
      "Best Params : {'n_nonzero_coefs': 30}\n"
     ]
    }
   ],
   "source": [
    "m8_params1 = {\n",
    "    'n_nonzero_coefs': [n for n in range(30, 150, 20)]\n",
    "}\n",
    "\n",
    "max_score=0\n",
    "m8_best_t = 0\n",
    "best_model8 = \"\"\n",
    "m8_best_grid_1 = \"\"\n",
    "\n",
    "for t in [0, 0.05, 0.1, 0.2, 0.25, 0.3, 0.45, 0.4, 0.45, 0.5, 0.6] :\n",
    "    scorer2 = make_scorer(new_scorer, threshold=t)\n",
    "    m8_grid_1 = GridSearchCV(model8, param_grid=m8_params1, scoring=scorer2, cv=2, verbose=0, n_jobs=-1)\n",
    "    m8_grid_1.fit(X_train, y_train)\n",
    "\n",
    "    if max_score < m8_grid_1.best_score_ :\n",
    "        best_model8 = m8_grid_1.best_estimator_\n",
    "        m8_best_t = t\n",
    "        m8_best_grid_1 = m8_grid_1\n",
    "        \n",
    "m8_grid_1 = m8_best_grid_1\n",
    "best_model8 = m8_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m8_grid_1.best_score_))     \n",
    "print(\"Threshold :\", m8_best_t)\n",
    "print(\"Best Params : {}\".format(m8_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LARS lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model9 = LassoLars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.4166666666666667\n",
      "Threshold : 0.6\n",
      "Best Params : {'alpha': 0.1, 'max_iter': 800}\n"
     ]
    }
   ],
   "source": [
    "m9_params1 = {\n",
    "    'alpha': [0.1, 1, 2, 5, 10, 20, 50, 100],\n",
    "    'max_iter' : [n for n in range(800, 1601, 200)]\n",
    "}\n",
    "\n",
    "max_score=0\n",
    "m9_best_t = 0\n",
    "best_model9 = \"\"\n",
    "m9_best_grid_1 = \"\"\n",
    "for t in [0, 0.05, 0.1, 0.2, 0.25, 0.3, 0.45, 0.4, 0.45, 0.5, 0.6] :\n",
    "    scorer2 = make_scorer(new_scorer, threshold=t)\n",
    "    m9_grid_1 = GridSearchCV(model9, param_grid=m9_params1, scoring=scorer2, cv=2, verbose=0, n_jobs=-1)\n",
    "    m9_grid_1.fit(X_train, y_train)\n",
    "\n",
    "    if max_score < m9_grid_1.best_score_ :\n",
    "        best_model9 = m9_grid_1.best_estimator_\n",
    "        m9_best_t = t\n",
    "        m9_best_grid_1 = m9_grid_1\n",
    "\n",
    "m9_grid_1 = m9_best_grid_1\n",
    "best_model9 = m9_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m9_grid_1.best_score_))     \n",
    "print(\"Threshold :\", m9_best_t)\n",
    "print(\"Best Params : {}\".format(m9_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.888888888888889\n",
      "Best Params : {'max_depth': 3, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "m10_params1 = {\n",
    "    'max_depth' : [None, 3, 5, 7, 9],\n",
    "    'n_estimators' : [10, 50, 100, 300, 500]\n",
    "}\n",
    "\n",
    "m10_grid_1 = GridSearchCV(model10, param_grid=m10_params1, scoring=scorer, cv=2, verbose=0, n_jobs=-1)\n",
    "m10_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model10 = m10_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m10_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m10_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 is stacked\n",
      "model 3 is stacked\n",
      "model 4 is stacked\n",
      "model 5 is stacked\n",
      "model 7 is stacked\n",
      "model 10 is stacked\n"
     ]
    }
   ],
   "source": [
    "models = [best_model1, best_model2, best_model3, best_model4, best_model5, best_model6, best_model7, best_model8, best_model9, best_model10]\n",
    "S_train = stacking(models, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.5555555555555556\n",
      "Best Params : {'colsample_bytree': 0.5, 'gamma': 1.5, 'learning_rate': 0.01, 'max_depth': 2, 'min_child_weight': 0.5, 'n_estimators': 200, 'probability': True, 'subsample': 0.5}\n",
      "Best Score : 1.0\n",
      "Best Params : {'C': 1, 'max_iter': 100}\n",
      "Epoch 1/30\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.6209 - acc: 0.5000\n",
      "Epoch 2/30\n",
      "6/6 [==============================] - 0s 233us/step - loss: 0.6157 - acc: 0.5000\n",
      "Epoch 3/30\n",
      "6/6 [==============================] - 0s 364us/step - loss: 0.6106 - acc: 0.5000\n",
      "Epoch 4/30\n",
      "6/6 [==============================] - 0s 357us/step - loss: 0.6054 - acc: 0.5000\n",
      "Epoch 5/30\n",
      "6/6 [==============================] - 0s 188us/step - loss: 0.6002 - acc: 0.5000\n",
      "Epoch 6/30\n",
      "6/6 [==============================] - 0s 194us/step - loss: 0.5951 - acc: 0.5000\n",
      "Epoch 7/30\n",
      "6/6 [==============================] - 0s 192us/step - loss: 0.5900 - acc: 0.5000\n",
      "Epoch 8/30\n",
      "6/6 [==============================] - 0s 236us/step - loss: 0.5850 - acc: 0.5000\n",
      "Epoch 9/30\n",
      "6/6 [==============================] - 0s 215us/step - loss: 0.5800 - acc: 0.5000\n",
      "Epoch 10/30\n",
      "6/6 [==============================] - 0s 220us/step - loss: 0.5750 - acc: 0.5000\n",
      "Epoch 11/30\n",
      "6/6 [==============================] - 0s 173us/step - loss: 0.5699 - acc: 0.6667\n",
      "Epoch 12/30\n",
      "6/6 [==============================] - 0s 186us/step - loss: 0.5646 - acc: 0.6667\n",
      "Epoch 13/30\n",
      "6/6 [==============================] - 0s 227us/step - loss: 0.5593 - acc: 0.6667\n",
      "Epoch 14/30\n",
      "6/6 [==============================] - 0s 184us/step - loss: 0.5540 - acc: 0.6667\n",
      "Epoch 15/30\n",
      "6/6 [==============================] - 0s 169us/step - loss: 0.5487 - acc: 1.0000\n",
      "Epoch 16/30\n",
      "6/6 [==============================] - 0s 153us/step - loss: 0.5433 - acc: 1.0000\n",
      "Epoch 17/30\n",
      "6/6 [==============================] - 0s 278us/step - loss: 0.5379 - acc: 1.0000\n",
      "Epoch 18/30\n",
      "6/6 [==============================] - 0s 189us/step - loss: 0.5325 - acc: 1.0000\n",
      "Epoch 19/30\n",
      "6/6 [==============================] - 0s 214us/step - loss: 0.5271 - acc: 1.0000\n",
      "Epoch 20/30\n",
      "6/6 [==============================] - 0s 216us/step - loss: 0.5219 - acc: 1.0000\n",
      "Epoch 21/30\n",
      "6/6 [==============================] - 0s 210us/step - loss: 0.5166 - acc: 1.0000\n",
      "Epoch 22/30\n",
      "6/6 [==============================] - 0s 206us/step - loss: 0.5113 - acc: 1.0000\n",
      "Epoch 23/30\n",
      "6/6 [==============================] - 0s 226us/step - loss: 0.5060 - acc: 1.0000\n",
      "Epoch 24/30\n",
      "6/6 [==============================] - 0s 217us/step - loss: 0.5006 - acc: 1.0000\n",
      "Epoch 25/30\n",
      "6/6 [==============================] - 0s 247us/step - loss: 0.4953 - acc: 1.0000\n",
      "Epoch 26/30\n",
      "6/6 [==============================] - 0s 201us/step - loss: 0.4899 - acc: 1.0000\n",
      "Epoch 27/30\n",
      "6/6 [==============================] - 0s 211us/step - loss: 0.4846 - acc: 1.0000\n",
      "Epoch 28/30\n",
      "6/6 [==============================] - 0s 220us/step - loss: 0.4794 - acc: 1.0000\n",
      "Epoch 29/30\n",
      "6/6 [==============================] - 0s 226us/step - loss: 0.4742 - acc: 1.0000\n",
      "Epoch 30/30\n",
      "6/6 [==============================] - 0s 202us/step - loss: 0.4690 - acc: 1.0000\n",
      "Epoch 1/30\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.0422 - acc: 0.5000\n",
      "Epoch 2/30\n",
      "6/6 [==============================] - 0s 373us/step - loss: 1.0388 - acc: 0.5000\n",
      "Epoch 3/30\n",
      "6/6 [==============================] - 0s 189us/step - loss: 1.0354 - acc: 0.5000\n",
      "Epoch 4/30\n",
      "6/6 [==============================] - 0s 163us/step - loss: 1.0320 - acc: 0.5000\n",
      "Epoch 5/30\n",
      "6/6 [==============================] - 0s 184us/step - loss: 1.0285 - acc: 0.5000\n",
      "Epoch 6/30\n",
      "6/6 [==============================] - 0s 183us/step - loss: 1.0251 - acc: 0.5000\n",
      "Epoch 7/30\n",
      "6/6 [==============================] - 0s 168us/step - loss: 1.0217 - acc: 0.5000\n",
      "Epoch 8/30\n",
      "6/6 [==============================] - 0s 173us/step - loss: 1.0184 - acc: 0.5000\n",
      "Epoch 9/30\n",
      "6/6 [==============================] - 0s 185us/step - loss: 1.0150 - acc: 0.5000\n",
      "Epoch 10/30\n",
      "6/6 [==============================] - 0s 173us/step - loss: 1.0116 - acc: 0.5000\n",
      "Epoch 11/30\n",
      "6/6 [==============================] - 0s 160us/step - loss: 1.0083 - acc: 0.5000\n",
      "Epoch 12/30\n",
      "6/6 [==============================] - 0s 174us/step - loss: 1.0050 - acc: 0.5000\n",
      "Epoch 13/30\n",
      "6/6 [==============================] - 0s 148us/step - loss: 1.0016 - acc: 0.5000\n",
      "Epoch 14/30\n",
      "6/6 [==============================] - 0s 173us/step - loss: 0.9983 - acc: 0.5000\n",
      "Epoch 15/30\n",
      "6/6 [==============================] - 0s 189us/step - loss: 0.9950 - acc: 0.5000\n",
      "Epoch 16/30\n",
      "6/6 [==============================] - 0s 154us/step - loss: 0.9917 - acc: 0.5000\n",
      "Epoch 17/30\n",
      "6/6 [==============================] - 0s 172us/step - loss: 0.9884 - acc: 0.5000\n",
      "Epoch 18/30\n",
      "6/6 [==============================] - 0s 154us/step - loss: 0.9852 - acc: 0.5000\n",
      "Epoch 19/30\n",
      "6/6 [==============================] - 0s 180us/step - loss: 0.9819 - acc: 0.5000\n",
      "Epoch 20/30\n",
      "6/6 [==============================] - 0s 161us/step - loss: 0.9787 - acc: 0.5000\n",
      "Epoch 21/30\n",
      "6/6 [==============================] - 0s 169us/step - loss: 0.9755 - acc: 0.5000\n",
      "Epoch 22/30\n",
      "6/6 [==============================] - 0s 151us/step - loss: 0.9723 - acc: 0.5000\n",
      "Epoch 23/30\n",
      "6/6 [==============================] - 0s 146us/step - loss: 0.9690 - acc: 0.5000\n",
      "Epoch 24/30\n",
      "6/6 [==============================] - 0s 219us/step - loss: 0.9659 - acc: 0.5000\n",
      "Epoch 25/30\n",
      "6/6 [==============================] - 0s 148us/step - loss: 0.9627 - acc: 0.5000\n",
      "Epoch 26/30\n",
      "6/6 [==============================] - 0s 203us/step - loss: 0.9595 - acc: 0.5000\n",
      "Epoch 27/30\n",
      "6/6 [==============================] - 0s 203us/step - loss: 0.9564 - acc: 0.5000\n",
      "Epoch 28/30\n",
      "6/6 [==============================] - 0s 199us/step - loss: 0.9532 - acc: 0.5000\n",
      "Epoch 29/30\n",
      "6/6 [==============================] - 0s 221us/step - loss: 0.9501 - acc: 0.5000\n",
      "Epoch 30/30\n",
      "6/6 [==============================] - 0s 185us/step - loss: 0.9470 - acc: 0.5000\n",
      "         m1        m3        m4        m5   m7  m10       xgb        lr  \\\n",
      "0  0.627895  0.412264  0.840000  0.939852  0.0  1.0  0.519123  0.761384   \n",
      "1  0.380666  0.021290  0.173333  0.003000  0.0  0.0  0.502097  0.388655   \n",
      "2  0.380666  0.065262  0.243333  0.009088  0.0  0.0  0.502097  0.398499   \n",
      "3  0.380666  0.107045  0.186667  0.011328  0.0  0.0  0.502097  0.397584   \n",
      "4  0.627895  0.429969  0.850000  0.938274  0.0  1.0  0.519123  0.762839   \n",
      "5  0.627895  0.997757  0.836667  0.998973  0.0  1.0  0.519123  0.799960   \n",
      "\n",
      "         NN    weight  xgb_b  lr_b  NN)b  weight_b  y  \n",
      "0  0.733069  0.293005    0.0   0.0   0.0       0.0  1  \n",
      "1  0.464986  0.403724    0.0   0.0   0.0       0.0  0  \n",
      "2  0.469057  0.372494    0.0   0.0   0.0       0.0  0  \n",
      "3  0.472350  0.377103    0.0   0.0   0.0       0.0  0  \n",
      "4  0.734920  0.286647    0.0   1.0   1.0       1.0  1  \n",
      "5  0.765756  0.177271    0.0   1.0   1.0       1.0  1  \n"
     ]
    }
   ],
   "source": [
    "meta_xgb = stacking_xgb(S_train, y_train, cv=2)\n",
    "meta_logistic = stacking_logistic(S_train, y_train, cv=2)\n",
    "meta_NN = stacking_NN(S_train, y_train, cv=2)\n",
    "meta_weight = stacking_weight(S_train, y_train, cv=2)\n",
    "\n",
    "y_pred_lst = []\n",
    "y_pred_binary_lst =[]\n",
    "threshold = \"auto\"\n",
    "for meta in [meta_xgb, meta_logistic, meta_NN, meta_weight] :\n",
    "    pred = meta.predict_proba(S_train)[:, 1]\n",
    "    y_pred_lst.append(pred)\n",
    "    y_pred_binary_lst.append(pred_to_binary(pred, threshold = threshold))\n",
    "\n",
    "print(making_result(S_train, y_pred_lst, y_pred_binary_lst, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_model1, open(path+'/model/model1.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model2, open(path+'/model/model2.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model3, open(path+'/model/model3.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model4, open(path+'/model/model4.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model5, open(path+'/model/model5.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model6, open(path+'/model/model6.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model7, open(path+'/model/model7.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model8, open(path+'/model/model8.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model9, open(path+'/model/model9.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model10, open(path+'/model/model10.pickle.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(meta_xgb, open(path+'/model/meta_xgb.pickle.dat', 'wb'))\n",
    "pickle.dump(meta_logistic, open(path+'/model/meta_logistic.pickle.dat', 'wb'))\n",
    "\n",
    "meta_NN.model.save_weights(path+'/model/meta_NN.h5')\n",
    "with open(path+'/model/meta_NN.json', 'w') as f :\n",
    "    f.write(meta_NN.model.to_json())\n",
    "    \n",
    "meta_weight.model.save_weights(path+'/model/meta_weight.h5')\n",
    "with open(path+'/model/meta_weight.json', 'w') as f :\n",
    "    f.write(meta_weight.model.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = pickle.load(open(path+'/model/model1.pickle.dat', 'rb'))\n",
    "model2 = pickle.load(open(path+'/model/model2.pickle.dat', 'rb'))\n",
    "model3 = pickle.load(open(path+'/model/model3.pickle.dat', 'rb'))\n",
    "model4 = pickle.load(open(path+'/model/model4.pickle.dat', 'rb'))\n",
    "model5 = pickle.load(open(path+'/model/model5.pickle.dat', 'rb'))\n",
    "model6 = pickle.load(open(path+'/model/model6.pickle.dat', 'rb'))\n",
    "model7 = pickle.load(open(path+'/model/model7.pickle.dat', 'rb'))\n",
    "model8 = pickle.load(open(path+'/model/model8.pickle.dat', 'rb'))\n",
    "model9 = pickle.load(open(path+'/model/model9.pickle.dat', 'rb'))\n",
    "model10 = pickle.load(open(path+'/model/model10.pickle.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_xgb = pickle.load(open(path+'/model/meta_xgb.pickle.dat', 'rb'))\n",
    "meta_logistic = pickle.load(open(path+'/model/meta_logistic.pickle.dat', 'rb'))\n",
    "\n",
    "with open(path+'/model/meta_NN.json', 'r') as f :\n",
    "    meta_NN = model_from_json(f.read())\n",
    "meta_NN.model.load_weights(path+'/model/meta_NN.h5')\n",
    "\n",
    "with open(path+'/model/meta_weight.json', 'r') as f :\n",
    "    meta_weight = model_from_json(f.read())\n",
    "meta_weight.model.load_weights(path+'/model/meta_weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 is stacked\n",
      "model 3 is stacked\n",
      "model 4 is stacked\n",
      "model 5 is stacked\n",
      "model 7 is stacked\n",
      "model 10 is stacked\n",
      "\n",
      "---------- Inference ----------\n",
      "Threshold : auto\n",
      "         m1        m3        m4        m5   m7  m10       xgb        lr  \\\n",
      "0  0.627895  0.412264  0.840000  0.939852  0.0  1.0  0.519123  0.761384   \n",
      "1  0.380666  0.021290  0.173333  0.003000  0.0  0.0  0.502097  0.388655   \n",
      "2  0.380666  0.065262  0.243333  0.009088  0.0  0.0  0.502097  0.398499   \n",
      "3  0.380666  0.107045  0.186667  0.011328  0.0  0.0  0.502097  0.397584   \n",
      "4  0.627895  0.429969  0.850000  0.938274  0.0  1.0  0.519123  0.762839   \n",
      "5  0.627895  0.997757  0.836667  0.998973  0.0  1.0  0.519123  0.799960   \n",
      "\n",
      "         NN    weight  xgb_b  lr_b  NN)b  weight_b  y  \n",
      "0  0.199917  0.293005    0.0   0.0   0.0       0.0  1  \n",
      "1  0.475704  0.403724    0.0   0.0   0.0       0.0  0  \n",
      "2  0.463122  0.372494    0.0   0.0   0.0       0.0  0  \n",
      "3  0.460205  0.377103    0.0   0.0   0.0       0.0  0  \n",
      "4  0.197574  0.286647    0.0   1.0   1.0       1.0  1  \n",
      "5  0.143750  0.177271    0.0   1.0   1.0       1.0  1  \n"
     ]
    }
   ],
   "source": [
    "models = [model1, model2, model3, model4, model5, model6, model7, model8, model9, model10]\n",
    "S_test = stacking(models, X_test)\n",
    "\n",
    "threshold = \"auto\"\n",
    "print(\"\\n---------- Inference ----------\")\n",
    "print(\"Threshold :\", threshold)\n",
    "\n",
    "y_pred_lst = []\n",
    "y_pred_binary_lst =[]\n",
    "for meta in [meta_xgb, meta_logistic, meta_NN, meta_weight] :\n",
    "    pred = meta.predict_proba(S_train)[:, 1]\n",
    "    y_pred_lst.append(pred)\n",
    "    y_pred_binary_lst.append(pred_to_binary(pred, threshold = threshold))\n",
    "\n",
    "final = export_csv(patient_num, y_pred_binary_lst, y_pred_lst, path = path, index=0)\n",
    "print(making_result(S_train, y_pred_lst, y_pred_binary_lst, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
