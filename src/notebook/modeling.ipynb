{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils.data_loader import train_data_loader, test_data_loader\n",
    "from utils.inference_tools import pred_to_binary, export_csv, making_result\n",
    "from utils.model_stacking import *\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, RidgeClassifier, SGDClassifier, Lars, LassoLars\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 07:50:31\n",
      "Author Name : KHW\n",
      "Model : ML Stacking\n",
      "Summary : HyperParams tuning with 10 sklearn models + 4 stacking model\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Current Time\n",
    "time = str(datetime.datetime.now()).split()[1].split('.')[0]\n",
    "print(\"Start:\", time)\n",
    "\n",
    "\n",
    "# Print Information\n",
    "name = 'KHW'\n",
    "model = 'ML Stacking'\n",
    "summary = 'HyperParams tuning with 10 sklearn models + 4 stacking model'\n",
    "\n",
    "print('Author Name :', name)\n",
    "print('Model :', model)\n",
    "print('Summary :', summary)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Setting\n",
    "path = \"./data\"\n",
    "pos_dir = path+\"/train/positive/\"\n",
    "neg_dir = path+\"/train/negative/\"\n",
    "test_dir = path+'/test/'\n",
    "\n",
    "features = ['firstorder', 'shape']\n",
    "target_voxel = (0.65, 0.65, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [1/3] Image of Positive Patient... (02:02:35)\n",
      ">>> Finished : Voxel Size Resampling (02:02:48)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (02:02:49)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [2/3] Image of Positive Patient... (02:02:51)\n",
      ">>> Finished : Voxel Size Resampling (02:03:04)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (02:03:05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [3/3] Image of Positive Patient... (02:03:07)\n",
      ">>> Finished : Voxel Size Resampling (02:03:19)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (02:03:20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [1/3] Image of Negative Patient... (02:03:23)\n",
      ">>> Finished : Voxel Size Resampling (02:03:35)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (02:03:36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [2/3] Image of Negative Patient... (02:03:39)\n",
      ">>> Finished : Voxel Size Resampling (02:03:53)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (02:03:53)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [3/3] Image of Negative Patient... (02:03:56)\n",
      ">>> Finished : Voxel Size Resampling (02:04:09)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (02:04:09)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created X of shape (6, 64) and y of shape (6,) (02:04:11)\n"
     ]
    }
   ],
   "source": [
    "do_n4 = False\n",
    "do_ws = True\n",
    "do_resample = True\n",
    "do_shuffle = True\n",
    "\n",
    "X_train, y_train = train_data_loader(pos_dir, neg_dir, do_n4, do_ws, do_resample, do_shuffle, features, target_voxel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [1/2] Image of Test Patient... (07:51:12)\n",
      ">>> Finished : Voxel Size Resampling (07:51:25)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>>Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (07:51:25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [2/2] Image of Test Patient... (07:52:14)\n",
      ">>> Finished : Voxel Size Resampling (07:52:28)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>>Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (07:52:29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    }
   ],
   "source": [
    "do_n4 = False\n",
    "do_ws = True\n",
    "do_resample = True\n",
    "\n",
    "X_test, patient_num, error_patient = test_data_loader(test_dir, do_n4, do_ws, do_resample, features, target_voxel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=4, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Model with Training Data\n",
    "model1 = XGBClassifier(n_jobs=4)\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Save model to file\n",
    "# pickle.dump(model1, open('./data/model/model1.pickle.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Model with Training Data\n",
    "model2 = SVC()\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Save model to file\n",
    "# pickle.dump(model2, open('./data/model/model2.pickle.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=4,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Model with Training Data\n",
    "model3 = LogisticRegression(n_jobs=4)\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Save model to file\n",
    "# pickle.dump(model3, open('./data/model/model3.pickle.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=4,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Model with Training Data\n",
    "model4 = RandomForestClassifier(n_jobs=4)\n",
    "model4.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Save model to file\n",
    "# pickle.dump(mode4l, open('./data/model/model4.pickle.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_scorer(y_true, y_pred, threshold=0.5) :\n",
    "    result = []\n",
    "    global BETA\n",
    "\n",
    "    for pred in list(y_pred) :\n",
    "        if pred >= threshold :\n",
    "            result.append(1)\n",
    "        else :\n",
    "            result.append(0)\n",
    "            \n",
    "    return fbeta_score(y_true, np.array(result), beta=BETA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = make_scorer(fbeta_score, beta=BETA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning & CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.0\n",
      "Best Params : {'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "m1_params1 = {\n",
    "    'max_depth' : [5,6,7,8],\n",
    "    'min_child_weight' : [0.5, 1, 5, 10, 15, 20],\n",
    "    'gamma' : [1.5, 2, 2.5, 3.0, 5],\n",
    "    'subsample' : [0.5, 0.6, 0.8, 1.0],\n",
    "    'colsample_bytree' : [0.5, 0.6, 0.8, 1.0],\n",
    "    'probability' : [True],\n",
    "    'learning_rate' : [0.01, 0.05, 0.1],\n",
    "    'n_estimators' : [300, 500, 700]\n",
    "\n",
    "}\n",
    "\n",
    "m1_grid_1 = GridSearchCV(model1, param_grid=m1_params1, scoring=scorer, cv=2, verbose=0, n_jobs=-1)\n",
    "m1_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model1 = m1_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m1_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m1_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.5555555555555556\n",
      "Best Params : {'C': 0.001, 'degree': 2, 'gamma': 0.001, 'probability': True}\n"
     ]
    }
   ],
   "source": [
    "m2_params1 = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100], \n",
    "    'gamma' : [0.001, 0.01, 0.1, 1, 2, 5, 10, 20],\n",
    "    'degree' : [2,3,4],\n",
    "    'probability' : [True]\n",
    "}\n",
    "\n",
    "m2_grid_1 = GridSearchCV(model2, param_grid=m2_params1, scoring=scorer, cv=2, verbose=0, n_jobs=-1)\n",
    "m2_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model2 = m2_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m2_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m2_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.5555555555555556\n",
      "Best Params : {'C': 0.001, 'max_iter': 100}\n"
     ]
    }
   ],
   "source": [
    "m3_params1 = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'max_iter' : [n for n in range(100,1101, 200)],\n",
    "}\n",
    "\n",
    "m3_grid_1 = GridSearchCV(model3, param_grid=m3_params1, scoring=scorer, cv=2, verbose=0, n_jobs=-1)\n",
    "m3_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model3 = m3_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m3_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m3_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.8095238095238094\n",
      "Best Params : {'max_depth': 40}\n"
     ]
    }
   ],
   "source": [
    "m4_params1 = {\n",
    "    'max_depth' : [6, 8, 10, 15, 20, 30, 40, 50],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5,10, 20, 50],\n",
    "    'n_estimators' : [100, 300, 500]\n",
    "}\n",
    "\n",
    "m4_grid_1 = GridSearchCV(model4, param_grid=m4_params1, scoring=scorer, cv=2, verbose=0, n_jobs=-1)\n",
    "m4_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model4 = m4_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m4_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m4_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.6666666666666666\n",
      "Best Params : {'C': 10, 'max_iter': 100, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "m5_params1 = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'max_iter' : [n for n in range(100,1101, 200)],\n",
    "    'penalty' : [\"l1\"]\n",
    "}\n",
    "\n",
    "m5_grid_1 = GridSearchCV(model5, param_grid=m5_params1, scoring=scorer, cv=2, verbose=0, n_jobs=-1)\n",
    "m5_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model5 = m5_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m5_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m5_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = RidgeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.42328042328042326\n",
      "Best Params : {'alpha': 0.1, 'max_iter': None}\n"
     ]
    }
   ],
   "source": [
    "m6_params1 = {\n",
    "    'alpha': [0.1, 1, 2, 5, 10, 20, 50, 100],\n",
    "    'max_iter' : [None]+[n for n in range(100,1101, 200)]\n",
    "}\n",
    "\n",
    "m6_grid_1 = GridSearchCV(model6, param_grid=m6_params1, scoring=scorer, cv=2, verbose=0, n_jobs=-1)\n",
    "m6_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model6 = m6_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m6_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m6_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### elasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.5555555555555556\n",
      "Best Params : {'alpha': 0.001, 'l1_ratio': 0.1, 'loss': 'log', 'max_iter': None, 'penalty': 'elasticnet'}\n"
     ]
    }
   ],
   "source": [
    "m7_params1 = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 2, 5, 10, 20, 50, 100],\n",
    "    'l1_ratio':[0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7], \n",
    "    'max_iter' : [None]+[n for n in range(800, 1601, 200)],\n",
    "    'penalty' : [\"elasticnet\"],\n",
    "    'loss' : [\"log\"]\n",
    "}\n",
    "\n",
    "m7_grid_1 = GridSearchCV(model7, param_grid=m7_params1, scoring=scorer, cv=2, verbose=0, n_jobs=-1)\n",
    "m7_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model7 = m7_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m7_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m7_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = Lars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.6349206349206349\n",
      "Threshold : 0.6\n",
      "Best Params : {'n_nonzero_coefs': 30}\n"
     ]
    }
   ],
   "source": [
    "m8_params1 = {\n",
    "    'n_nonzero_coefs': [n for n in range(30, 150, 20)]\n",
    "}\n",
    "\n",
    "max_score=0\n",
    "m8_best_t = 0\n",
    "best_model8 = \"\"\n",
    "m8_best_grid_1 = \"\"\n",
    "\n",
    "for t in [0, 0.05, 0.1, 0.2, 0.25, 0.3, 0.45, 0.4, 0.45, 0.5, 0.6] :\n",
    "    scorer2 = make_scorer(new_scorer, threshold=t)\n",
    "    m8_grid_1 = GridSearchCV(model8, param_grid=m8_params1, scoring=scorer2, cv=2, verbose=0, n_jobs=-1)\n",
    "    m8_grid_1.fit(X_train, y_train)\n",
    "\n",
    "    if max_score < m8_grid_1.best_score_ :\n",
    "        best_model8 = m8_grid_1.best_estimator_\n",
    "        m8_best_t = t\n",
    "        m8_best_grid_1 = m8_grid_1\n",
    "        \n",
    "m8_grid_1 = m8_best_grid_1\n",
    "best_model8 = m8_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m8_grid_1.best_score_))     \n",
    "print(\"Threshold :\", m8_best_t)\n",
    "print(\"Best Params : {}\".format(m8_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LARS lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model9 = LassoLars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.5277777777777778\n",
      "Threshold : 0.6\n",
      "Best Params : {'alpha': 0.1, 'max_iter': 800}\n"
     ]
    }
   ],
   "source": [
    "m9_params1 = {\n",
    "    'alpha': [0.1, 1, 2, 5, 10, 20, 50, 100],\n",
    "    'max_iter' : [n for n in range(800, 1601, 200)]\n",
    "}\n",
    "\n",
    "max_score=0\n",
    "m9_best_t = 0\n",
    "best_model9 = \"\"\n",
    "m9_best_grid_1 = \"\"\n",
    "for t in [0, 0.05, 0.1, 0.2, 0.25, 0.3, 0.45, 0.4, 0.45, 0.5, 0.6] :\n",
    "    scorer2 = make_scorer(new_scorer, threshold=t)\n",
    "    m9_grid_1 = GridSearchCV(model9, param_grid=m9_params1, scoring=scorer2, cv=2, verbose=0, n_jobs=-1)\n",
    "    m9_grid_1.fit(X_train, y_train)\n",
    "\n",
    "    if max_score < m9_grid_1.best_score_ :\n",
    "        best_model9 = m9_grid_1.best_estimator_\n",
    "        m9_best_t = t\n",
    "        m9_best_grid_1 = m9_grid_1\n",
    "\n",
    "m9_grid_1 = m9_best_grid_1\n",
    "best_model9 = m9_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m9_grid_1.best_score_))     \n",
    "print(\"Threshold :\", m9_best_t)\n",
    "print(\"Best Params : {}\".format(m9_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.8518518518518517\n",
      "Best Params : {'max_depth': 5, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "m10_params1 = {\n",
    "    'max_depth' : [None, 3, 5, 7, 9],\n",
    "    'n_estimators' : [10, 50, 100, 300, 500]\n",
    "}\n",
    "\n",
    "m10_grid_1 = GridSearchCV(model10, param_grid=m10_params1, scoring=scorer, cv=2, verbose=0, n_jobs=-1)\n",
    "m10_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model10 = m10_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m10_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m10_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 is stacked\n",
      "model 3 is stacked\n",
      "model 4 is stacked\n",
      "model 5 is stacked\n",
      "model 7 is stacked\n",
      "model 10 is stacked\n",
      "Best Score : 0.5555555555555556\n",
      "Best Params : {'colsample_bytree': 1.0, 'gamma': 1.5, 'learning_rate': 0.01, 'max_depth': 2, 'min_child_weight': 0.5, 'n_estimators': 100, 'probability': True, 'subsample': 0.5}\n",
      "Best Score : 1.0\n",
      "Best Params : {'C': 1, 'max_iter': 100}\n",
      "Epoch 1/30\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.6603 - acc: 0.8333\n",
      "Epoch 2/30\n",
      "6/6 [==============================] - 0s 240us/step - loss: 0.6537 - acc: 1.0000\n",
      "Epoch 3/30\n",
      "6/6 [==============================] - 0s 199us/step - loss: 0.6471 - acc: 1.0000\n",
      "Epoch 4/30\n",
      "6/6 [==============================] - 0s 173us/step - loss: 0.6405 - acc: 1.0000\n",
      "Epoch 5/30\n",
      "6/6 [==============================] - 0s 179us/step - loss: 0.6340 - acc: 1.0000\n",
      "Epoch 6/30\n",
      "6/6 [==============================] - 0s 203us/step - loss: 0.6276 - acc: 1.0000\n",
      "Epoch 7/30\n",
      "6/6 [==============================] - 0s 176us/step - loss: 0.6214 - acc: 1.0000\n",
      "Epoch 8/30\n",
      "6/6 [==============================] - 0s 191us/step - loss: 0.6151 - acc: 1.0000\n",
      "Epoch 9/30\n",
      "6/6 [==============================] - 0s 182us/step - loss: 0.6089 - acc: 1.0000\n",
      "Epoch 10/30\n",
      "6/6 [==============================] - 0s 168us/step - loss: 0.6027 - acc: 1.0000\n",
      "Epoch 11/30\n",
      "6/6 [==============================] - 0s 219us/step - loss: 0.5966 - acc: 1.0000\n",
      "Epoch 12/30\n",
      "6/6 [==============================] - 0s 182us/step - loss: 0.5906 - acc: 1.0000\n",
      "Epoch 13/30\n",
      "6/6 [==============================] - 0s 192us/step - loss: 0.5843 - acc: 1.0000\n",
      "Epoch 14/30\n",
      "6/6 [==============================] - 0s 189us/step - loss: 0.5780 - acc: 1.0000\n",
      "Epoch 15/30\n",
      "6/6 [==============================] - 0s 194us/step - loss: 0.5718 - acc: 1.0000\n",
      "Epoch 16/30\n",
      "6/6 [==============================] - 0s 194us/step - loss: 0.5656 - acc: 1.0000\n",
      "Epoch 17/30\n",
      "6/6 [==============================] - 0s 180us/step - loss: 0.5594 - acc: 1.0000\n",
      "Epoch 18/30\n",
      "6/6 [==============================] - 0s 176us/step - loss: 0.5532 - acc: 1.0000\n",
      "Epoch 19/30\n",
      "6/6 [==============================] - 0s 201us/step - loss: 0.5469 - acc: 1.0000\n",
      "Epoch 20/30\n",
      "6/6 [==============================] - 0s 191us/step - loss: 0.5407 - acc: 1.0000\n",
      "Epoch 21/30\n",
      "6/6 [==============================] - 0s 191us/step - loss: 0.5345 - acc: 1.0000\n",
      "Epoch 22/30\n",
      "6/6 [==============================] - 0s 193us/step - loss: 0.5283 - acc: 1.0000\n",
      "Epoch 23/30\n",
      "6/6 [==============================] - 0s 178us/step - loss: 0.5221 - acc: 1.0000\n",
      "Epoch 24/30\n",
      "6/6 [==============================] - 0s 186us/step - loss: 0.5159 - acc: 1.0000\n",
      "Epoch 25/30\n",
      "6/6 [==============================] - 0s 190us/step - loss: 0.5097 - acc: 1.0000\n",
      "Epoch 26/30\n",
      "6/6 [==============================] - 0s 181us/step - loss: 0.5036 - acc: 1.0000\n",
      "Epoch 27/30\n",
      "6/6 [==============================] - 0s 177us/step - loss: 0.4974 - acc: 1.0000\n",
      "Epoch 28/30\n",
      "6/6 [==============================] - 0s 176us/step - loss: 0.4912 - acc: 1.0000\n",
      "Epoch 29/30\n",
      "6/6 [==============================] - 0s 210us/step - loss: 0.4852 - acc: 1.0000\n",
      "Epoch 30/30\n",
      "6/6 [==============================] - 0s 193us/step - loss: 0.4793 - acc: 1.0000\n",
      "Epoch 1/30\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.5956 - acc: 0.5000\n",
      "Epoch 2/30\n",
      "6/6 [==============================] - 0s 153us/step - loss: 0.5945 - acc: 0.5000\n",
      "Epoch 3/30\n",
      "6/6 [==============================] - 0s 321us/step - loss: 0.5934 - acc: 0.5000\n",
      "Epoch 4/30\n",
      "6/6 [==============================] - 0s 166us/step - loss: 0.5923 - acc: 0.5000\n",
      "Epoch 5/30\n",
      "6/6 [==============================] - 0s 165us/step - loss: 0.5912 - acc: 0.5000\n",
      "Epoch 6/30\n",
      "6/6 [==============================] - 0s 185us/step - loss: 0.5901 - acc: 0.5000\n",
      "Epoch 7/30\n",
      "6/6 [==============================] - 0s 183us/step - loss: 0.5890 - acc: 0.5000\n",
      "Epoch 8/30\n",
      "6/6 [==============================] - 0s 170us/step - loss: 0.5879 - acc: 0.5000\n",
      "Epoch 9/30\n",
      "6/6 [==============================] - 0s 168us/step - loss: 0.5868 - acc: 0.5000\n",
      "Epoch 10/30\n",
      "6/6 [==============================] - 0s 161us/step - loss: 0.5857 - acc: 0.5000\n",
      "Epoch 11/30\n",
      "6/6 [==============================] - 0s 209us/step - loss: 0.5846 - acc: 0.5000\n",
      "Epoch 12/30\n",
      "6/6 [==============================] - 0s 182us/step - loss: 0.5835 - acc: 0.5000\n",
      "Epoch 13/30\n",
      "6/6 [==============================] - 0s 173us/step - loss: 0.5824 - acc: 0.5000\n",
      "Epoch 14/30\n",
      "6/6 [==============================] - 0s 177us/step - loss: 0.5814 - acc: 0.5000\n",
      "Epoch 15/30\n",
      "6/6 [==============================] - 0s 174us/step - loss: 0.5803 - acc: 0.5000\n",
      "Epoch 16/30\n",
      "6/6 [==============================] - 0s 177us/step - loss: 0.5792 - acc: 0.5000\n",
      "Epoch 17/30\n",
      "6/6 [==============================] - 0s 177us/step - loss: 0.5781 - acc: 0.5000\n",
      "Epoch 18/30\n",
      "6/6 [==============================] - 0s 176us/step - loss: 0.5770 - acc: 0.5000\n",
      "Epoch 19/30\n",
      "6/6 [==============================] - 0s 186us/step - loss: 0.5760 - acc: 0.5000\n",
      "Epoch 20/30\n",
      "6/6 [==============================] - 0s 185us/step - loss: 0.5749 - acc: 0.5000\n",
      "Epoch 21/30\n",
      "6/6 [==============================] - 0s 153us/step - loss: 0.5738 - acc: 0.5000\n",
      "Epoch 22/30\n",
      "6/6 [==============================] - 0s 192us/step - loss: 0.5727 - acc: 0.5000\n",
      "Epoch 23/30\n",
      "6/6 [==============================] - 0s 182us/step - loss: 0.5717 - acc: 0.5000\n",
      "Epoch 24/30\n",
      "6/6 [==============================] - 0s 174us/step - loss: 0.5706 - acc: 0.5000\n",
      "Epoch 25/30\n",
      "6/6 [==============================] - 0s 177us/step - loss: 0.5695 - acc: 0.5000\n",
      "Epoch 26/30\n",
      "6/6 [==============================] - 0s 179us/step - loss: 0.5685 - acc: 0.5000\n",
      "Epoch 27/30\n",
      "6/6 [==============================] - 0s 174us/step - loss: 0.5674 - acc: 0.5000\n",
      "Epoch 28/30\n",
      "6/6 [==============================] - 0s 164us/step - loss: 0.5663 - acc: 0.5000\n",
      "Epoch 29/30\n",
      "6/6 [==============================] - 0s 216us/step - loss: 0.5653 - acc: 0.5000\n",
      "Epoch 30/30\n",
      "6/6 [==============================] - 0s 172us/step - loss: 0.5642 - acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# layer1\n",
    "models = [best_model1, best_model2, best_model3, best_model4, best_model5, best_model6, best_model7, best_model8, best_model9, best_model10]\n",
    "S_train = stacking(models, X_train)\n",
    "\n",
    "meta_xgb = stacking_xgb(S_train, y_train, cv=2)\n",
    "meta_logistic = stacking_logistic(S_train, y_train, cv=2)\n",
    "meta_NN = stacking_NN(S_train, y_train, cv=2)\n",
    "meta_weight = stacking_weight(S_train, y_train, cv=2)\n",
    "\n",
    "y_pred_lst = []\n",
    "y_pred_binary_lst =[]\n",
    "threshold = \"auto\"\n",
    "for meta in [meta_xgb, meta_logistic, meta_NN, meta_weight] :\n",
    "    pred = meta.predict_proba(S_train)[:, 1]\n",
    "    y_pred_lst.append(pred)\n",
    "    y_pred_binary_lst.append(pred_to_binary(pred, threshold = threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 is stacked\n",
      "model 2 is stacked\n",
      "model 3 is stacked\n",
      "model 4 is stacked\n",
      "Best Score : 0.5555555555555556\n",
      "Best Params : {'colsample_bytree': 0.5, 'gamma': 1.5, 'learning_rate': 0.01, 'max_depth': 2, 'min_child_weight': 0.5, 'n_estimators': 200, 'probability': True, 'subsample': 0.8}\n",
      "Best Score : 1.0\n",
      "Best Params : {'C': 10, 'max_iter': 100}\n",
      "Epoch 1/30\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.8511 - acc: 0.5000\n",
      "Epoch 2/30\n",
      "6/6 [==============================] - 0s 198us/step - loss: 0.8433 - acc: 0.5000\n",
      "Epoch 3/30\n",
      "6/6 [==============================] - 0s 419us/step - loss: 0.8356 - acc: 0.5000\n",
      "Epoch 4/30\n",
      "6/6 [==============================] - 0s 354us/step - loss: 0.8282 - acc: 0.5000\n",
      "Epoch 5/30\n",
      "6/6 [==============================] - 0s 258us/step - loss: 0.8211 - acc: 0.5000\n",
      "Epoch 6/30\n",
      "6/6 [==============================] - 0s 162us/step - loss: 0.8141 - acc: 0.5000\n",
      "Epoch 7/30\n",
      "6/6 [==============================] - 0s 196us/step - loss: 0.8075 - acc: 0.5000\n",
      "Epoch 8/30\n",
      "6/6 [==============================] - 0s 173us/step - loss: 0.8008 - acc: 0.5000\n",
      "Epoch 9/30\n",
      "6/6 [==============================] - 0s 223us/step - loss: 0.7944 - acc: 0.5000\n",
      "Epoch 10/30\n",
      "6/6 [==============================] - 0s 385us/step - loss: 0.7884 - acc: 0.5000\n",
      "Epoch 11/30\n",
      "6/6 [==============================] - 0s 172us/step - loss: 0.7825 - acc: 0.5000\n",
      "Epoch 12/30\n",
      "6/6 [==============================] - 0s 164us/step - loss: 0.7773 - acc: 0.5000\n",
      "Epoch 13/30\n",
      "6/6 [==============================] - 0s 207us/step - loss: 0.7724 - acc: 0.5000\n",
      "Epoch 14/30\n",
      "6/6 [==============================] - 0s 210us/step - loss: 0.7679 - acc: 0.5000\n",
      "Epoch 15/30\n",
      "6/6 [==============================] - 0s 186us/step - loss: 0.7638 - acc: 0.5000\n",
      "Epoch 16/30\n",
      "6/6 [==============================] - 0s 171us/step - loss: 0.7600 - acc: 0.5000\n",
      "Epoch 17/30\n",
      "6/6 [==============================] - 0s 206us/step - loss: 0.7565 - acc: 0.5000\n",
      "Epoch 18/30\n",
      "6/6 [==============================] - 0s 168us/step - loss: 0.7531 - acc: 0.5000\n",
      "Epoch 19/30\n",
      "6/6 [==============================] - 0s 169us/step - loss: 0.7499 - acc: 0.5000\n",
      "Epoch 20/30\n",
      "6/6 [==============================] - 0s 191us/step - loss: 0.7469 - acc: 0.5000\n",
      "Epoch 21/30\n",
      "6/6 [==============================] - 0s 461us/step - loss: 0.7440 - acc: 0.5000\n",
      "Epoch 22/30\n",
      "6/6 [==============================] - 0s 201us/step - loss: 0.7413 - acc: 0.5000\n",
      "Epoch 23/30\n",
      "6/6 [==============================] - 0s 253us/step - loss: 0.7387 - acc: 0.5000\n",
      "Epoch 24/30\n",
      "6/6 [==============================] - 0s 203us/step - loss: 0.7362 - acc: 0.5000\n",
      "Epoch 25/30\n",
      "6/6 [==============================] - 0s 193us/step - loss: 0.7338 - acc: 0.5000\n",
      "Epoch 26/30\n",
      "6/6 [==============================] - 0s 205us/step - loss: 0.7316 - acc: 0.5000\n",
      "Epoch 27/30\n",
      "6/6 [==============================] - 0s 205us/step - loss: 0.7294 - acc: 0.5000\n",
      "Epoch 28/30\n",
      "6/6 [==============================] - 0s 209us/step - loss: 0.7274 - acc: 0.5000\n",
      "Epoch 29/30\n",
      "6/6 [==============================] - 0s 179us/step - loss: 0.7254 - acc: 0.5000\n",
      "Epoch 30/30\n",
      "6/6 [==============================] - 0s 190us/step - loss: 0.7235 - acc: 0.5000\n",
      "Epoch 1/30\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.6653 - acc: 0.5000\n",
      "Epoch 2/30\n",
      "6/6 [==============================] - 0s 523us/step - loss: 0.6646 - acc: 0.5000\n",
      "Epoch 3/30\n",
      "6/6 [==============================] - 0s 144us/step - loss: 0.6639 - acc: 0.5000\n",
      "Epoch 4/30\n",
      "6/6 [==============================] - 0s 127us/step - loss: 0.6632 - acc: 0.5000\n",
      "Epoch 5/30\n",
      "6/6 [==============================] - 0s 142us/step - loss: 0.6625 - acc: 0.5000\n",
      "Epoch 6/30\n",
      "6/6 [==============================] - 0s 156us/step - loss: 0.6618 - acc: 0.5000\n",
      "Epoch 7/30\n",
      "6/6 [==============================] - 0s 149us/step - loss: 0.6611 - acc: 0.5000\n",
      "Epoch 8/30\n",
      "6/6 [==============================] - 0s 157us/step - loss: 0.6605 - acc: 0.5000\n",
      "Epoch 9/30\n",
      "6/6 [==============================] - 0s 144us/step - loss: 0.6598 - acc: 0.5000\n",
      "Epoch 10/30\n",
      "6/6 [==============================] - 0s 143us/step - loss: 0.6592 - acc: 0.5000\n",
      "Epoch 11/30\n",
      "6/6 [==============================] - 0s 143us/step - loss: 0.6586 - acc: 0.5000\n",
      "Epoch 12/30\n",
      "6/6 [==============================] - 0s 146us/step - loss: 0.6579 - acc: 0.5000\n",
      "Epoch 13/30\n",
      "6/6 [==============================] - 0s 142us/step - loss: 0.6573 - acc: 0.5000\n",
      "Epoch 14/30\n",
      "6/6 [==============================] - 0s 138us/step - loss: 0.6567 - acc: 0.5000\n",
      "Epoch 15/30\n",
      "6/6 [==============================] - 0s 175us/step - loss: 0.6561 - acc: 0.5000\n",
      "Epoch 16/30\n",
      "6/6 [==============================] - 0s 202us/step - loss: 0.6556 - acc: 0.5000\n",
      "Epoch 17/30\n",
      "6/6 [==============================] - 0s 239us/step - loss: 0.6550 - acc: 0.5000\n",
      "Epoch 18/30\n",
      "6/6 [==============================] - 0s 197us/step - loss: 0.6544 - acc: 0.5000\n",
      "Epoch 19/30\n",
      "6/6 [==============================] - 0s 188us/step - loss: 0.6538 - acc: 0.5000\n",
      "Epoch 20/30\n",
      "6/6 [==============================] - 0s 197us/step - loss: 0.6533 - acc: 0.5000\n",
      "Epoch 21/30\n",
      "6/6 [==============================] - 0s 210us/step - loss: 0.6527 - acc: 0.5000\n",
      "Epoch 22/30\n",
      "6/6 [==============================] - 0s 167us/step - loss: 0.6522 - acc: 0.5000\n",
      "Epoch 23/30\n",
      "6/6 [==============================] - 0s 187us/step - loss: 0.6516 - acc: 0.5000\n",
      "Epoch 24/30\n",
      "6/6 [==============================] - 0s 183us/step - loss: 0.6511 - acc: 0.5000\n",
      "Epoch 25/30\n",
      "6/6 [==============================] - 0s 186us/step - loss: 0.6505 - acc: 0.5000\n",
      "Epoch 26/30\n",
      "6/6 [==============================] - 0s 161us/step - loss: 0.6500 - acc: 0.5000\n",
      "Epoch 27/30\n",
      "6/6 [==============================] - 0s 207us/step - loss: 0.6495 - acc: 0.5000\n",
      "Epoch 28/30\n",
      "6/6 [==============================] - 0s 194us/step - loss: 0.6490 - acc: 0.5000\n",
      "Epoch 29/30\n",
      "6/6 [==============================] - 0s 192us/step - loss: 0.6484 - acc: 0.5000\n",
      "Epoch 30/30\n",
      "6/6 [==============================] - 0s 169us/step - loss: 0.6479 - acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# layer2\n",
    "models2 = [meta_xgb, meta_logistic, meta_NN, meta_weight]\n",
    "S_train2 = stacking(models2, S_train, layer=2)\n",
    "\n",
    "meta_xgb2 = stacking_xgb(S_train2, y_train, cv=2)\n",
    "meta_logistic2 = stacking_logistic(S_train2, y_train, cv=2)\n",
    "meta_NN2 = stacking_NN(S_train2, y_train, cv=2)\n",
    "meta_weight2 = stacking_weight(S_train2, y_train, cv=2)\n",
    "\n",
    "y_pred_lst2 = []\n",
    "y_pred_binary_lst2 =[]\n",
    "threshold = \"auto\"\n",
    "for meta in [meta_xgb2, meta_logistic2, meta_NN2, meta_weight2] :\n",
    "    pred = meta.predict_proba(S_train2)[:, 1]\n",
    "    y_pred_lst2.append(pred)\n",
    "    y_pred_binary_lst2.append(pred_to_binary(pred, threshold = threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model\n",
    "meta_model2 = meta_weight2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    m1        m3   m4        m5   m7  m10       xgb        lr        NN  \\\n",
      "0  0.5  0.993480  0.8  0.999988  0.0  1.0  0.502207  0.793455  0.732167   \n",
      "1  0.5  0.439779  0.7  0.999837  0.0  1.0  0.502207  0.758269  0.657518   \n",
      "2  0.5  0.086069  0.1  0.000119  0.0  0.0  0.494128  0.373469  0.421195   \n",
      "3  0.5  0.054889  0.3  0.000024  0.0  0.0  0.494128  0.386006  0.448127   \n",
      "4  0.5  0.390369  0.9  0.999975  0.0  1.0  0.502207  0.766915  0.673472   \n",
      "5  0.5  0.105163  0.2  0.000005  0.0  0.0  0.494128  0.382234  0.436628   \n",
      "\n",
      "     weight ...  weight_b      xgb2       lr2       NN2   weight2  xgb_b2  \\\n",
      "0  0.847067 ...       0.0  0.592395  0.730635  0.439951  0.672961     0.0   \n",
      "1  0.881083 ...       0.0  0.592395  0.699041  0.442964  0.670809     0.0   \n",
      "2  0.608616 ...       0.0  0.400482  0.345026  0.466465  0.589058     0.0   \n",
      "3  0.653813 ...       0.0  0.400482  0.368206  0.464042  0.596622     0.0   \n",
      "4  0.901112 ...       1.0  0.592395  0.711026  0.441642  0.674311     0.0   \n",
      "5  0.625972 ...       1.0  0.400482  0.357603  0.465197  0.592590     0.0   \n",
      "\n",
      "   lr_b2  NN_b2  weight_b2  y  \n",
      "0    0.0    0.0        0.0  1  \n",
      "1    0.0    0.0        0.0  1  \n",
      "2    0.0    0.0        0.0  0  \n",
      "3    0.0    0.0        0.0  0  \n",
      "4    1.0    1.0        1.0  1  \n",
      "5    1.0    1.0        1.0  0  \n",
      "\n",
      "[6 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "print(making_result(S_train, y_pred_lst, y_pred_binary_lst, y_pred_lst2, y_pred_binary_lst2, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_model1, open(path+'/model/model1.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model2, open(path+'/model/model2.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model3, open(path+'/model/model3.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model4, open(path+'/model/model4.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model5, open(path+'/model/model5.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model6, open(path+'/model/model6.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model7, open(path+'/model/model7.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model8, open(path+'/model/model8.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model9, open(path+'/model/model9.pickle.dat', 'wb'))\n",
    "pickle.dump(best_model10, open(path+'/model/model10.pickle.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(meta_xgb, open(path+'/model/meta_xgb.pickle.dat', 'wb'))\n",
    "pickle.dump(meta_logistic, open(path+'/model/meta_logistic.pickle.dat', 'wb'))\n",
    "\n",
    "meta_NN.model.save_weights(path+'/model/meta_NN.h5')\n",
    "with open(path+'/model/meta_NN.json', 'w') as f :\n",
    "    f.write(meta_NN.model.to_json())\n",
    "    \n",
    "meta_weight.model.save_weights(path+'/model/meta_weight.h5')\n",
    "with open(path+'/model/meta_weight.json', 'w') as f :\n",
    "    f.write(meta_weight.model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(meta_xgb2, open(path+'/model/meta_xgb2.pickle.dat', 'wb'))\n",
    "pickle.dump(meta_logistic2, open(path+'/model/meta_logistic2.pickle.dat', 'wb'))\n",
    "\n",
    "meta_NN2.model.save_weights(path+'/model/meta_NN2.h5')\n",
    "with open(path+'/model/meta_NN2.json', 'w') as f :\n",
    "    f.write(meta_NN2.model.to_json())\n",
    "    \n",
    "meta_weight2.model.save_weights(path+'/model/meta_weight2.h5')\n",
    "with open(path+'/model/meta_weight2.json', 'w') as f :\n",
    "    f.write(meta_weight2.model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model2.model.save_weights(path+'/model/meta_model2.h5')\n",
    "with open(path+'/model/meta_model2.json', 'w') as f :\n",
    "    f.write(meta_model2.model.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = pickle.load(open(path+'/model/model1.pickle.dat', 'rb'))\n",
    "model2 = pickle.load(open(path+'/model/model2.pickle.dat', 'rb'))\n",
    "model3 = pickle.load(open(path+'/model/model3.pickle.dat', 'rb'))\n",
    "model4 = pickle.load(open(path+'/model/model4.pickle.dat', 'rb'))\n",
    "model5 = pickle.load(open(path+'/model/model5.pickle.dat', 'rb'))\n",
    "model6 = pickle.load(open(path+'/model/model6.pickle.dat', 'rb'))\n",
    "model7 = pickle.load(open(path+'/model/model7.pickle.dat', 'rb'))\n",
    "model8 = pickle.load(open(path+'/model/model8.pickle.dat', 'rb'))\n",
    "model9 = pickle.load(open(path+'/model/model9.pickle.dat', 'rb'))\n",
    "model10 = pickle.load(open(path+'/model/model10.pickle.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_xgb = pickle.load(open(path+'/model/meta_xgb.pickle.dat', 'rb'))\n",
    "meta_logistic = pickle.load(open(path+'/model/meta_logistic.pickle.dat', 'rb'))\n",
    "\n",
    "with open(path+'/model/meta_NN.json', 'r') as f :\n",
    "    meta_NN = model_from_json(f.read())\n",
    "meta_NN.model.load_weights(path+'/model/meta_NN.h5')\n",
    "\n",
    "with open(path+'/model/meta_weight.json', 'r') as f :\n",
    "    meta_weight = model_from_json(f.read())\n",
    "meta_weight.model.load_weights(path+'/model/meta_weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_xgb2 = pickle.load(open(path+'/model/meta_xgb2.pickle.dat', 'rb'))\n",
    "meta_logistic2 = pickle.load(open(path+'/model/meta_logistic2.pickle.dat', 'rb'))\n",
    "\n",
    "with open(path+'/model/meta_NN2.json', 'r') as f :\n",
    "    meta_NN2 = model_from_json(f.read())\n",
    "meta_NN2.model.load_weights(path+'/model/meta_NN2.h5')\n",
    "\n",
    "with open(path+'/model/meta_weight2.json', 'r') as f :\n",
    "    meta_weight2 = model_from_json(f.read())\n",
    "meta_weight2.model.load_weights(path+'/model/meta_weight2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'/model/meta_model2.json', 'r') as f :\n",
    "    meta_model2 = model_from_json(f.read())\n",
    "meta_model2.model.load_weights(path+'/model/meta_weight2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_binary(pred_array, threshold = 0.5):\n",
    "    if threshold == \"auto\" :\n",
    "        \n",
    "        pred_binary = sorted(list(pred_array))\n",
    "        threshold = pred_binary[int(len(pred_binary)*4/10)]\n",
    "        pred_binary = np.array(pred_binary)\n",
    "        pred_binary[pred_binary > threshold] = 1\n",
    "        pred_binary[pred_binary <= threshold] = 0\n",
    "        \n",
    "    else :\n",
    "        pred_binary = np.copy(pred_array)\n",
    "        pred_binary[pred_binary > threshold] = 1\n",
    "        pred_binary[pred_binary <= threshold] = 0\n",
    "\n",
    "    return pred_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- Inference ----------\n",
      "Threshold : auto\n",
      "model 1 is stacked\n",
      "model 3 is stacked\n",
      "model 4 is stacked\n",
      "model 10 is stacked\n",
      "model 1 is stacked\n",
      "model 2 is stacked\n",
      "model 3 is stacked\n",
      "model 4 is stacked\n",
      "    m1        m3    m4  m10       xgb        lr        NN    weight  xgb_b  \\\n",
      "0  0.5  0.000003  0.47  0.5  0.513582  0.565334  0.591972  0.360199    0.0   \n",
      "1  0.5  0.561660  0.63  0.5  0.576999  0.576369  0.607946  0.363784    1.0   \n",
      "\n",
      "   lr_b  NN_b  weight_b      xgb2       lr2       NN2   weight2  xgb_b2  \\\n",
      "0   0.0   0.0       0.0  0.594461  0.550571  0.555390  0.355301     0.0   \n",
      "1   1.0   1.0       1.0  0.594461  0.573189  0.551675  0.355165     0.0   \n",
      "\n",
      "   lr_b2  NN_b2  weight_b2    y  \n",
      "0    0.0    0.0        0.0  0.0  \n",
      "1    1.0    1.0        1.0  1.0  \n"
     ]
    }
   ],
   "source": [
    "models = [model1, model2, model3, model4, model5, model6, model7, model8, model9, model10]\n",
    "models2 = [meta_xgb, meta_logistic, meta_NN, meta_weight]\n",
    "models3 = [meta_xgb2, meta_logistic2, meta_NN2, meta_weight2]\n",
    "\n",
    "threshold = \"auto\"\n",
    "print(\"\\n---------- Inference ----------\")\n",
    "print(\"Threshold :\", threshold)\n",
    "\n",
    "S_test = stacking(models, X_test)\n",
    "y_pred_lst = []\n",
    "y_pred_binary_lst =[]\n",
    "for meta in models2 :\n",
    "    pred = meta.predict_proba(S_test)[:, 1]\n",
    "    y_pred_lst.append(pred)\n",
    "    y_pred_binary_lst.append(pred_to_binary(pred, threshold = threshold))\n",
    "\n",
    "S_test2 = stacking(models2, S_test, layer=2)\n",
    "y_pred_lst2 = []\n",
    "y_pred_binary_lst2 =[]\n",
    "threshold = \"auto\"\n",
    "for meta in models3 :\n",
    "    pred = meta.predict_proba(S_test2)[:, 1]\n",
    "    y_pred_lst2.append(pred)\n",
    "    y_pred_binary_lst2.append(pred_to_binary(pred, threshold = threshold))\n",
    "\n",
    "final, final_df = export_csv(patient_num, error_patient, y_pred_binary_lst2, y_pred_lst2, path = path, index=3)\n",
    "print(making_result(S_test, y_pred_lst, y_pred_binary_lst, y_pred_lst2, y_pred_binary_lst2, final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
