{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils.data_loader import train_data_loader, test_data_loader\n",
    "#from utils.inference_tools import pred_to_binary, export_csv\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, RidgeClassifier, SGDClassifier, Lars, LassoLars\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 16:58:47\n",
      "Author Name : KHW\n",
      "Model : Stacking\n",
      "Summary : HyperParams tuning with 9 sklearn models\n",
      "\n",
      "\n",
      "Processing [1/3] Image of Positive Patient... (16:58:47)\n",
      ">>> Finished : Voxel Size Resampling (16:58:58)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (16:59:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [2/3] Image of Positive Patient... (16:59:03)\n",
      ">>> Finished : Voxel Size Resampling (16:59:14)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (16:59:15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [3/3] Image of Positive Patient... (16:59:17)\n",
      ">>> Finished : Voxel Size Resampling (16:59:28)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (16:59:29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [1/3] Image of Negative Patient... (16:59:32)\n",
      ">>> Finished : Voxel Size Resampling (16:59:43)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (16:59:44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [2/3] Image of Negative Patient... (16:59:47)\n",
      ">>> Finished : Voxel Size Resampling (16:59:59)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (17:00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [3/3] Image of Negative Patient... (17:00:03)\n",
      ">>> Finished : Voxel Size Resampling (17:00:14)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>> Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (17:00:15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created X of shape (6, 186) and y of shape (6,) (17:00:18)\n",
      "Processing [1/2] Image of Test Patient... (17:00:18)\n",
      ">>> Finished : Voxel Size Resampling (17:00:29)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>>Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (17:00:31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing [2/2] Image of Test Patient... (17:00:34)\n",
      ">>> Finished : Voxel Size Resampling (17:00:44)\n",
      ">>> Unique Value of BRAIN mask : [0. 1.]\n",
      ">>>Unique Value of INFARCT mask : [0. 1.]\n",
      ">>> Finished : White-stripe Normalization (17:00:46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    }
   ],
   "source": [
    "# Print Current Time\n",
    "time = str(datetime.datetime.now()).split()[1].split('.')[0]\n",
    "print(\"Start:\", time)\n",
    "\n",
    "\n",
    "# Print Information\n",
    "name = 'KHW'\n",
    "model = 'Stacking'\n",
    "summary = 'HyperParams tuning with 9 sklearn models'\n",
    "\n",
    "print('Author Name :', name)\n",
    "print('Model :', model)\n",
    "print('Summary :', summary)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Setting\n",
    "pos_dir = \"../data/train/positive/\"\n",
    "neg_dir = \"../data/train/negative/\"\n",
    "test_dir = '../data/test/'\n",
    "\n",
    "do_n4 = False\n",
    "do_ws = True\n",
    "do_resample = True\n",
    "\n",
    "do_shuffle_train = True\n",
    "do_shuffle_test = False\n",
    "save_to_disk = False\n",
    "return_patient_num_train = False\n",
    "return_patient_num_test = True\n",
    "\n",
    "\n",
    "# Data Load\n",
    "X_train, y_train = train_data_loader(pos_dir, neg_dir, do_n4, do_ws, do_resample, do_shuffle_train, save_to_disk, return_patient_num_train)\n",
    "X_test, patient_num = test_data_loader(test_dir, do_n4, do_ws, do_resample, do_shuffle_test, save_to_disk, return_patient_num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit Model with Training Data\n",
    "model1 = xgb.XGBClassifier(n_jobs=4)\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Save model to file\n",
    "pickle.dump(model1, open('../data/model/model1.pickle.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit Model with Training Data\n",
    "model2 = SVC()\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Save model to file\n",
    "pickle.dump(model2, open('../data/model/model2.pickle.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit Model with Training Data\n",
    "model3 = LogisticRegression(n_jobs=4)\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Save model to file\n",
    "pickle.dump(model3, open('../data/model/model3.pickle.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit Model with Training Data\n",
    "model4 = RandomForestClassifier(n_jobs=4)\n",
    "model4.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Save model to file\n",
    "pickle.dump(mode4l, open('../data/model/model4.pickle.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_scorer(y_true, y_pred, threshold=0.5) :\n",
    "    result = []\n",
    "\n",
    "    for pred in list(y_pred) :\n",
    "        if pred >= threshold :\n",
    "            result.append(1)\n",
    "        else :\n",
    "            result.append(0)\n",
    "            \n",
    "    return fbeta_score(y_true, np.array(result), beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scorer = make_scorer(fbeta_score, beta=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning & CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.3703703703703704\n",
      "Best Params : {'colsample_bytree': 0.5, 'gamma': 0, 'max_depth': 3, 'min_child_weight': 0.5, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "m1_params1 = {\n",
    "    'max_depth' : [3,4,5,7,9],\n",
    "    'min_child_weight' : [0.5, 1],\n",
    "    'gamma' : [0, 0.1, 0.5, 1, 1.5, 2, 5],\n",
    "    'subsample' : [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree' : [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'probability' : True\n",
    "}\n",
    "\n",
    "m1_grid_1 = GridSearchCV(model1, param_grid=m1_params1, scoring=scorer, cv=2, verbose=0, n_jobs=4)\n",
    "m1_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model1 = m1_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m1_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m1_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.5555555555555556\n",
      "Best Params : {'learning_rate': 0.01, 'n_estimators': 1100}\n"
     ]
    }
   ],
   "source": [
    "m1_params2 = {\n",
    "    'learning_rate' : [0.01, 0.05, 0.07, 0.1, 0.2],\n",
    "    'n_estimators' : [n for n in range(500,1501,200)]\n",
    "}\n",
    "\n",
    "m1_grid_2 = GridSearchCV(best_model1, param_grid=m1_params2, scoring=scorer, cv=2, verbose=0, n_jobs=4)\n",
    "m1_grid_2.fit(X_train, y_train)\n",
    "\n",
    "best_model1 = m1_grid_2.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m1_grid_2.best_score_))\n",
    "print(\"Best Params : {}\".format(m1_grid_2.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.5555555555555556\n",
      "Best Params : {'C': 0.001, 'degree': 2, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "m2_params1 = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100], \n",
    "    'gamma' : [0.001, 0.01, 0.1, 1, 2, 5, 10, 20],\n",
    "    'degree' : [2,3,4],\n",
    "    'probability' : True\n",
    "}\n",
    "\n",
    "m2_grid_1 = GridSearchCV(model2, param_grid=m2_params1, scoring=scorer, cv=2, verbose=0, n_jobs=4)\n",
    "m2_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model2 = m2_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m2_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m2_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.1851851851851852\n",
      "Best Params : {'C': 0.001, 'max_iter': 100}\n"
     ]
    }
   ],
   "source": [
    "m3_params1 = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'max_iter' : [n for n in range(100,1101, 200)],\n",
    "    'probability' : True\n",
    "}\n",
    "\n",
    "m3_grid_1 = GridSearchCV(model3, param_grid=m3_params1, scoring=scorer, cv=2, verbose=0, n_jobs=4)\n",
    "m3_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model3 = m3_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m3_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m3_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model4 = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.5555555555555556\n",
      "Best Params : {'max_depth': 10, 'min_samples_leaf': 3}\n"
     ]
    }
   ],
   "source": [
    "m4_params1 = {\n",
    "    'max_depth' : [n for n in range(10, 51, 10)],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5,10, 20, 50],\n",
    "    'probability' : True\n",
    "}\n",
    "\n",
    "m4_grid_1 = GridSearchCV(model4, param_grid=m4_params1, scoring=scorer, cv=2, verbose=0, n_jobs=4)\n",
    "m4_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model4 = m4_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m4_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m4_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.5555555555555556\n",
      "Best Params : {'n_estimators': 800}\n"
     ]
    }
   ],
   "source": [
    "m4_params2 = {\n",
    "    'n_estimators' : [n for n in range(100,1001,100)]\n",
    "}\n",
    "\n",
    "m4_grid_2 = GridSearchCV(best_model4, param_grid=m4_params2, scoring=scorer, cv=2, verbose=0, n_jobs=4)\n",
    "m4_grid_2.fit(X_train, y_train)\n",
    "\n",
    "best_model4 = m4_grid_2.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m4_grid_2.best_score_))\n",
    "print(\"Best Params : {}\".format(m4_grid_2.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model5 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.7407407407407408\n",
      "Best Params : {'C': 0.001, 'max_iter': 100, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "m5_params1 = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'max_iter' : [n for n in range(100,1101, 200)],\n",
    "    'penalty' : [\"l1\"],\n",
    "    'probability' : True\n",
    "}\n",
    "\n",
    "m5_grid_1 = GridSearchCV(model5, param_grid=m5_params1, scoring=scorer, cv=2, verbose=0, n_jobs=4)\n",
    "m5_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model5 = m3_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m5_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m5_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model6 = RidgeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.0\n",
      "Best Params : {'alpha': 0.1, 'max_iter': None}\n"
     ]
    }
   ],
   "source": [
    "m6_params1 = {\n",
    "    'alpha': [0.1, 1, 2, 5, 10, 20, 50, 100],\n",
    "    'max_iter' : [None]+[n for n in range(100,1101, 200)],\n",
    "    'probability' : True\n",
    "}\n",
    "\n",
    "m6_grid_1 = GridSearchCV(model6, param_grid=m6_params1, scoring=scorer, cv=2, verbose=0, n_jobs=4)\n",
    "m6_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model6 = m6_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m6_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m6_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### elasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model7 = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.5714285714285714\n",
      "Best Params : {'alpha': 50, 'l1_ratio': 0.6, 'max_iter': 800, 'penalty': 'elasticnet'}\n"
     ]
    }
   ],
   "source": [
    "m7_params1 = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 2, 5, 10, 20, 50, 100],\n",
    "    'l1_ratio':[0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7], \n",
    "    'max_iter' : [None]+[n for n in range(800, 1601, 200)],\n",
    "    'penalty' : [\"elasticnet\"],\n",
    "    'probability' : True\n",
    "}\n",
    "\n",
    "m7_grid_1 = GridSearchCV(model7, param_grid=m7_params1, scoring=scorer, cv=2, verbose=0, n_jobs=4)\n",
    "m7_grid_1.fit(X_train, y_train)\n",
    "\n",
    "best_model7 = m7_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m7_grid_1.best_score_))\n",
    "print(\"Best Params : {}\".format(m7_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model8 = Lars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.4166666666666667\n",
      "Threshold : 0.6\n",
      "Best Params : {'n_nonzero_coefs': 30}\n"
     ]
    }
   ],
   "source": [
    "m8_params1 = {\n",
    "    'n_nonzero_coefs': [n for n in range(30, 150, 20)],\n",
    "    'probability' : True\n",
    "}\n",
    "\n",
    "max_score=0\n",
    "m8_best_t = 0\n",
    "best_model8 = \"\"\n",
    "m8_best_grid_1 = \"\"\n",
    "\n",
    "for t in [0, 0.05, 0.1, 0.2, 0.25, 0.3, 0.45, 0.4, 0.45, 0.5, 0.6] :\n",
    "    scorer2 = make_scorer(new_scorer, threshold=t)\n",
    "    m8_grid_1 = GridSearchCV(model8, param_grid=m8_params1, scoring=scorer2, cv=2, verbose=0, n_jobs=4)\n",
    "    m8_grid_1.fit(X_train, y_train)\n",
    "\n",
    "    if max_score < m8_grid_1.best_score_ :\n",
    "        best_model8 = m8_grid_1.best_estimator_\n",
    "        m8_best_t = t\n",
    "        m8_best_grid_1 = m8_grid_1\n",
    "        \n",
    "m8_grid_1 = m8_best_grid_1\n",
    "best_model8 = m8_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m8_grid_1.best_score_))     \n",
    "print(\"Threshold :\", m8_best_t)\n",
    "print(\"Best Params : {}\".format(m8_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LARS lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model9 = LassoLars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.4166666666666667\n",
      "Threshold : 0.6\n",
      "Best Params : {'alpha': 0.1, 'max_iter': 800}\n"
     ]
    }
   ],
   "source": [
    "m9_params1 = {\n",
    "    'alpha': [0.1, 1, 2, 5, 10, 20, 50, 100],\n",
    "    'max_iter' : [n for n in range(800, 1601, 200)],\n",
    "    'probability' : True\n",
    "}\n",
    "\n",
    "max_score=0\n",
    "m9_best_t = 0\n",
    "best_model9 = \"\"\n",
    "m9_best_grid_1 = \"\"\n",
    "for t in [0, 0.05, 0.1, 0.2, 0.25, 0.3, 0.45, 0.4, 0.45, 0.5, 0.6] :\n",
    "    scorer2 = make_scorer(new_scorer, threshold=t)\n",
    "    m9_grid_1 = GridSearchCV(model9, param_grid=m9_params1, scoring=scorer2, cv=2, verbose=0, n_jobs=4)\n",
    "    m9_grid_1.fit(X_train, y_train)\n",
    "\n",
    "    if max_score < m9_grid_1.best_score_ :\n",
    "        best_model9 = m9_grid_1.best_estimator_\n",
    "        m9_best_t = t\n",
    "        m9_best_grid_1 = m9_grid_1\n",
    "\n",
    "m9_grid_1 = m9_best_grid_1\n",
    "best_model9 = m9_grid_1.best_estimator_\n",
    "\n",
    "print(\"Best Score : {}\".format(m9_grid_1.best_score_))     \n",
    "print(\"Threshold :\", m9_best_t)\n",
    "print(\"Best Params : {}\".format(m9_grid_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stacking(models, data) : \n",
    "    result = []\n",
    "    \n",
    "    for model in models :\n",
    "        result.append(model.predict_proba(data))\n",
    "        \n",
    "    return np.array(result).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [best_model1, best_model2, best_model3, best_model4, best_model5, best_model6, best_model7, best_model8, best_model9]\n",
    "S_train = stacking(models, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stack_fn(num_models=9):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=num_models, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimi\n",
    "                  \n",
    "                  zer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.3829 - acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f271702a8d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_model = KerasClassifier(build_fn=stack_fn)\n",
    "meta_model.fit(S_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(model1, open('../data/model/model1.pickle.dat', 'wb'))\n",
    "pickle.dump(model2, open('../data/model/model2.pickle.dat', 'wb'))\n",
    "pickle.dump(model3, open('../data/model/model3.pickle.dat', 'wb'))\n",
    "pickle.dump(model4, open('../data/model/model4.pickle.dat', 'wb'))\n",
    "pickle.dump(model5, open('../data/model/model5.pickle.dat', 'wb'))\n",
    "pickle.dump(model6, open('../data/model/model6.pickle.dat', 'wb'))\n",
    "pickle.dump(model7, open('../data/model/model7.pickle.dat', 'wb'))\n",
    "pickle.dump(model8, open('../data/model/model8.pickle.dat', 'wb'))\n",
    "pickle.dump(model9, open('../data/model/model9.pickle.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_model.model.save_weights('../data/model/model_weights.h5')\n",
    "\n",
    "with open('../data/model/model_architecture.json', 'w') as f :\n",
    "    f.write(meta_model.model.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator SVC from version 0.20.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.20.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.20.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.20.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator RidgeClassifier from version 0.20.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator SGDClassifier from version 0.20.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator Lars from version 0.20.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator LassoLars from version 0.20.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py:110: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
      "  warnings.warn('`Sequential.model` is deprecated. '\n"
     ]
    }
   ],
   "source": [
    "model1 = pickle.load(open('../data/model/model1.pickle.dat', 'rb'))\n",
    "model2 = pickle.load(open('../data/model/model2.pickle.dat', 'rb'))\n",
    "model3 = pickle.load(open('../data/model/model3.pickle.dat', 'rb'))\n",
    "model4 = pickle.load(open('../data/model/model4.pickle.dat', 'rb'))\n",
    "model5 = pickle.load(open('../data/model/model5.pickle.dat', 'rb'))\n",
    "model6 = pickle.load(open('../data/model/model6.pickle.dat', 'rb'))\n",
    "model7 = pickle.load(open('../data/model/model7.pickle.dat', 'rb'))\n",
    "model8 = pickle.load(open('../data/model/model8.pickle.dat', 'rb'))\n",
    "model9 = pickle.load(open('../data/model/model9.pickle.dat', 'rb'))\n",
    "\n",
    "with open('../data/model/model_architecture.json', 'r') as f :\n",
    "    meta = model_from_json(f.read())\n",
    "\n",
    "meta.model.load_weights('../data/model/model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make Predictions for Test Data\n",
    "models = [model1, model2, model3, model4, model5, model6, model7, model8, model9]\n",
    "S_test = stacking(models, X_test)\n",
    "\n",
    "threshold = 0.65\n",
    "y_pred = meta.predict_proba(S_test)[:, 1]\n",
    "y_pred_binary = pred_to_binary(y_pred, threshold = threshold)\n",
    "\n",
    "\n",
    "# Make 'output.csv'\n",
    "export_csv(patient_num, y_pred_binary, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
